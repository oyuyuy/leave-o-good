"""
è¯•è¯•æˆ‘è¯•è¯•å¥½ - è§†é¢‘ä¸‹è½½å·¥å…· Web åç«¯
"""

import os
import re
import uuid
import shutil
import requests
import threading
import time
import urllib.parse
from flask import Flask, request, jsonify, send_file, send_from_directory
from flask_cors import CORS

app = Flask(__name__, static_folder='static')
CORS(app)

# é…ç½®
DOWNLOAD_FOLDER = os.path.join(os.path.dirname(__file__), 'downloads')
ZIP_FOLDER = os.path.join(os.path.dirname(__file__), 'zips')
STATIC_FOLDER = os.path.join(os.path.dirname(__file__), 'static')

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)
os.makedirs(ZIP_FOLDER, exist_ok=True)
os.makedirs(STATIC_FOLDER, exist_ok=True)

# æ–‡ä»¶æ¸…ç†æ—¶é—´ï¼ˆç§’ï¼‰
CLEANUP_INTERVAL = 300  # 5åˆ†é’Ÿåæ¸…ç†


def cleanup_old_files():
    """æ¸…ç†è¿‡æœŸçš„ä¸‹è½½æ–‡ä»¶å’ŒZIPåŒ…"""
    while True:
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        now = time.time()

        # æ¸…ç†downloadsç›®å½•
        if os.path.exists(DOWNLOAD_FOLDER):
            for folder in os.listdir(DOWNLOAD_FOLDER):
                folder_path = os.path.join(DOWNLOAD_FOLDER, folder)
                if os.path.isdir(folder_path):
                    if now - os.path.getmtime(folder_path) > CLEANUP_INTERVAL:
                        shutil.rmtree(folder_path, ignore_errors=True)

        # æ¸…ç†zipsç›®å½•
        if os.path.exists(ZIP_FOLDER):
            for f in os.listdir(ZIP_FOLDER):
                file_path = os.path.join(ZIP_FOLDER, f)
                if os.path.isfile(file_path):
                    if now - os.path.getmtime(file_path) > CLEANUP_INTERVAL:
                        os.remove(file_path)


# å¯åŠ¨æ¸…ç†çº¿ç¨‹
cleanup_thread = threading.Thread(target=cleanup_old_files, daemon=True)
cleanup_thread.start()


@app.route('/')
def index():
    """è¿”å›ä¸»é¡µ"""
    return send_from_directory('.', 'index.html')


@app.route('/static/<path:filename>')
def serve_static(filename):
    """æä¾›é™æ€æ–‡ä»¶"""
    return send_from_directory(STATIC_FOLDER, filename)


@app.route('/api/parse', methods=['POST'])
def parse_content():
    """
    è§£æç”¨æˆ·ç²˜è´´çš„HTMLå†…å®¹ï¼Œæå–è§†é¢‘URL
    é’ˆå¯¹oiioiiå¹³å°ä¼˜åŒ–
    """
    try:
        data = request.get_json()
        content = data.get('content', '')

        if not content:
            return jsonify({'code': 400, 'msg': 'å†…å®¹ä¸ºç©º', 'data': None})

        video_urls = []

        # ===== ä¼˜å…ˆåŒ¹é… oiioii å¹³å°çš„ hogi://video/ è§†é¢‘URI =====
        # åŒ¹é…URLç¼–ç æ ¼å¼: hogi%3A%2F%2Fvideo%2Fxxx.mp4
        hogi_encoded_pattern = r'hogi%3A%2F%2Fvideo%2F([a-zA-Z0-9_]+\.mp4)'
        for video_id in re.findall(hogi_encoded_pattern, content, re.IGNORECASE):
            url = f'https://api.oiioii.ai/res/read_file?uri=hogi%3A%2F%2Fvideo%2F{video_id}'
            if url not in video_urls:
                video_urls.append(url)

        # åŒ¹é…æœªç¼–ç æ ¼å¼: hogi://video/xxx.mp4
        hogi_raw_pattern = r'hogi://video/([a-zA-Z0-9_]+\.mp4)'
        for video_id in re.findall(hogi_raw_pattern, content, re.IGNORECASE):
            url = f'https://api.oiioii.ai/res/read_file?uri=hogi%3A%2F%2Fvideo%2F{video_id}'
            if url not in video_urls:
                video_urls.append(url)

        # ===== å…œåº•ï¼šåŒ¹é…é€šç”¨ mp4 ç›´é“¾ =====
        if not video_urls:
            exclude_keywords = [
                'thumb', 'thumbnail', 'preview', 'poster', 'cover',
                'icon', 'logo', 'avatar', 'img', 'image', 'photo',
                '.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg',
                'watermark', 'sprite', 'loading', 'placeholder',
                'first_frame'
            ]

            patterns = [
                r'["\']([^"\']*?\.mp4(?:\?[^"\']*)?)["\']',
                r'src\s*=\s*["\']([^"\']+\.mp4[^"\']*)["\']',
                r'data-src\s*=\s*["\']([^"\']+\.mp4[^"\']*)["\']',
            ]

            for pattern in patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for url in matches:
                    url = url.strip()
                    if url.startswith('//'):
                        url = 'https:' + url
                    if not url.startswith('http'):
                        continue

                    url_lower = url.lower()
                    is_excluded = False
                    for kw in exclude_keywords:
                        if kw in url_lower:
                            is_excluded = True
                            break
                    if is_excluded:
                        continue
                    if len(url) < 30:
                        continue
                    if url not in video_urls:
                        video_urls.append(url)

        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        unique_urls = []
        for url in video_urls:
            # ç”¨è§†é¢‘æ–‡ä»¶åå»é‡ï¼ˆæå–hogiè§†é¢‘IDæˆ–URLè·¯å¾„ï¼‰
            parsed = urllib.parse.urlparse(url)
            uri_param = urllib.parse.parse_qs(parsed.query).get('uri', [''])[0]
            dedup_key = uri_param if uri_param else url.split('?')[0]
            if dedup_key not in seen:
                seen.add(dedup_key)
                unique_urls.append(url)

        return jsonify({
            'code': 200,
            'msg': 'success',
            'data': {
                'video_urls': unique_urls,
                'count': len(unique_urls)
            }
        })

    except Exception as e:
        return jsonify({'code': 500, 'msg': str(e), 'data': None})


@app.route('/api/download', methods=['POST'])
def download_videos():
    """
    ä¸‹è½½é€‰ä¸­çš„è§†é¢‘å¹¶æ‰“åŒ…ä¸ºZIP
    """
    try:
        data = request.get_json()
        selected_links = data.get('selected_links', [])
        custom_names = data.get('custom_names', {})

        if not selected_links:
            return jsonify({'code': 400, 'msg': 'æœªé€‰æ‹©è§†é¢‘', 'data': None})

        logs = []
        task_id = str(uuid.uuid4())[:8]
        task_folder = os.path.join(DOWNLOAD_FOLDER, task_id)
        os.makedirs(task_folder, exist_ok=True)

        logs.append(f"ğŸ“¦ åˆ›å»ºä¸‹è½½ä»»åŠ¡: {task_id}")

        # ä¸‹è½½æ¯ä¸ªè§†é¢‘
        downloaded_files = []
        for idx, url in enumerate(selected_links):
            try:
                # è·å–è‡ªå®šä¹‰åç§°
                name = custom_names.get(str(idx), f'è§†é¢‘{idx + 1}')
                filename = f"{name}.mp4"
                filepath = os.path.join(task_folder, filename)

                logs.append(f"â¬‡ æ­£åœ¨ä¸‹è½½: {name}")

                # ä¸‹è½½è§†é¢‘ - ä½¿ç”¨æ›´å®Œæ•´çš„è¯·æ±‚å¤´
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Referer': 'https://www.oiioii.ai/',
                    'Origin': 'https://www.oiioii.ai',
                    'Accept': '*/*',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'identity',
                    'Connection': 'keep-alive',
                }

                response = requests.get(url, headers=headers, stream=True, timeout=180, allow_redirects=True)
                response.raise_for_status()

                # æ£€æŸ¥Content-Typeæ˜¯å¦ä¸ºè§†é¢‘
                content_type = response.headers.get('Content-Type', '')
                content_length = int(response.headers.get('Content-Length', 0))

                # å¦‚æœæ–‡ä»¶å¤ªå°ï¼ˆå°äº10KBï¼‰ï¼Œå¯èƒ½ä¸æ˜¯çœŸæ­£çš„è§†é¢‘
                if content_length > 0 and content_length < 10240:
                    logs.append(f"âš  è·³è¿‡ {name}: æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½ä¸æ˜¯è§†é¢‘")
                    continue

                # å†™å…¥æ–‡ä»¶
                total_size = 0
                with open(filepath, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=65536):
                        if chunk:
                            f.write(chunk)
                            total_size += len(chunk)

                # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶
                if total_size < 10240:
                    os.remove(filepath)
                    logs.append(f"âš  è·³è¿‡ {name}: ä¸‹è½½çš„æ–‡ä»¶å¤ªå°")
                    continue

                # æ£€æŸ¥æ–‡ä»¶å¤´æ˜¯å¦ä¸ºè§†é¢‘æ ¼å¼
                with open(filepath, 'rb') as f:
                    header = f.read(12)
                    # MP4æ–‡ä»¶é€šå¸¸åœ¨ç¬¬4-8å­—èŠ‚åŒ…å« 'ftyp'
                    is_mp4 = b'ftyp' in header or header.startswith(b'\x00\x00\x00')
                    # WebMæ–‡ä»¶ä»¥ 0x1A45DFA3 å¼€å¤´
                    is_webm = header.startswith(b'\x1a\x45\xdf\xa3')
                    # å¦‚æœæ˜¯oiioiiçš„APIé“¾æ¥ï¼Œæ”¾å®½æ£€æŸ¥ï¼ˆAPIå¯èƒ½è¿”å›æœ‰æ•ˆè§†é¢‘ä½†æ ¼å¼å¤´ä¸åŒï¼‰
                    is_oiioii_api = 'oiioii' in url.lower()

                    if not is_mp4 and not is_webm and not is_oiioii_api:
                        os.remove(filepath)
                        logs.append(f"âš  è·³è¿‡ {name}: æ–‡ä»¶æ ¼å¼æ— æ•ˆ")
                        continue

                downloaded_files.append(filepath)
                size_mb = total_size / (1024 * 1024)
                logs.append(f"âœ” ä¸‹è½½å®Œæˆ: {name} ({size_mb:.1f}MB)")

            except Exception as e:
                logs.append(f"âŒ ä¸‹è½½å¤±è´¥ ({name}): {str(e)}")

        if not downloaded_files:
            shutil.rmtree(task_folder, ignore_errors=True)
            return jsonify({
                'code': 500,
                'msg': 'æ‰€æœ‰è§†é¢‘ä¸‹è½½å¤±è´¥',
                'data': {'logs': logs}
            })

        # æ‰“åŒ…ä¸ºZIP
        logs.append("ğŸ“¦ æ­£åœ¨æ‰“åŒ…è§†é¢‘...")
        zip_id = f"{task_id}"
        zip_path = os.path.join(ZIP_FOLDER, zip_id)
        shutil.make_archive(zip_path, 'zip', task_folder)

        # æ¸…ç†ä¸´æ—¶ä¸‹è½½æ–‡ä»¶å¤¹
        shutil.rmtree(task_folder, ignore_errors=True)

        logs.append(f"âœ” æ‰“åŒ…å®Œæˆï¼Œå…± {len(downloaded_files)} ä¸ªè§†é¢‘")

        return jsonify({
            'code': 200,
            'msg': 'success',
            'data': {
                'zip_id': f"{zip_id}.zip",
                'logs': logs
            }
        })

    except Exception as e:
        return jsonify({'code': 500, 'msg': str(e), 'data': None})


@app.route('/api/zip/<zip_id>')
def get_zip(zip_id):
    """æä¾›ZIPæ–‡ä»¶ä¸‹è½½"""
    try:
        zip_path = os.path.join(ZIP_FOLDER, zip_id)
        if os.path.exists(zip_path):
            return send_file(
                zip_path,
                as_attachment=True,
                download_name=f"è§†é¢‘åˆé›†_{zip_id}"
            )
        else:
            return jsonify({'code': 404, 'msg': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
    except Exception as e:
        return jsonify({'code': 500, 'msg': str(e)}), 500


@app.route('/api/get_invite_code', methods=['GET'])
def get_invite_code():
    """
    è·å–é‚€è¯·ç ï¼ˆç¤ºä¾‹å®ç°ï¼‰
    ä½ å¯ä»¥æ ¹æ®å®é™…éœ€æ±‚ä¿®æ”¹è¿™ä¸ªé€»è¾‘
    """
    try:
        # è¿™é‡Œå¯ä»¥å®ç°ä½ è‡ªå·±çš„é‚€è¯·ç é€»è¾‘
        # æ¯”å¦‚ä»æ•°æ®åº“è·å–ã€è°ƒç”¨ç¬¬ä¸‰æ–¹APIç­‰
        invite_code = f"XYGS-{uuid.uuid4().hex[:8].upper()}"

        return jsonify({
            'code': 200,
            'msg': 'success',
            'data': {
                'invite_code': invite_code
            }
        })
    except Exception as e:
        return jsonify({'code': 500, 'msg': str(e), 'data': None})


if __name__ == '__main__':
    print("=" * 50)
    print("è¿å·¥ä½œå®¤ - è§†é¢‘ä¸‹è½½å·¥å…· Web 1.0")
    print("=" * 50)
    print(f"æœåŠ¡å¯åŠ¨ä¸­...")
    print(f"è®¿é—®åœ°å€: http://localhost:5000")
    print("=" * 50)

    # ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ waitress æˆ– gunicorn
    app.run(host='0.0.0.0', port=5000, debug=False)
